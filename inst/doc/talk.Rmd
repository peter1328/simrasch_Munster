---
title: "Statistical and Theoretical Reductionism in Research on Scientific Thinking"
subtitle: "How much can the Rasch model tell us?"
author: |
  | Peter A. Edelsbrunner
  | ETH Zurich, Learning Sciences
  | &
  | Fabian Dablander
  | University of Tübingen, Cognitive Science
output:
  ioslides_presentation:
    widescreen: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!--  
      For informations on ioslides syntax see http://rmarkdown.rstudio.com/ioslides_presentation_format.html
      To enable presenter mode add ?presentme=true to the URL of the presentation, for example:
      mypresentation.html?presentme=true
      The presenter mode window will open and will always re-open with the presentation until it's disabled with:
      mypresentation.html?presentme=false
      Presenter notes are embedded under
      <div class="notes">
      ending with
      </div>
-->

## Scientific Thinking in Children {.flexbox .vcenter }
<div class="notes">
      Scientific thinking entails various psychological processes that are needed for intentional knowledge seeking and the coordination of theory and evidence, which lie at the heart of the scientific process.
      [BULLET 1]For example, to find out whether researchers in scientific thinking can adequately use the Rasch model, a research question and hypotheses have to be formulated, a controlled experimental design has to be developed and conducted, data have to be interpreted and conclusions drawn.
      [BULLET 2] Since the early decades of educational and developmental psychology, researchers have been interested in the education of scientific thinking to raise responsible citizens, hence also in its development and its assessment.
      [BULLET 3] The first major wave of instrument development to assess scientific reasoning took place in the 1970s and 1980s.
      Following the inclusion of scienficic reasoning in the PISA and TIMMS assessment programs, a second wave of instrument development started in the 2000s. In this second wave, researchers started to apply the Rasch model in the process of instrument development.
- Hypothesis generation, evidence generation, evidence evaluation, drawing conclusions (Mayer, 2007).
- *Processes for intentional knowledge seeking and coordination of theory and evidence* (Mayer et al., 2014).
</div>

![structure](img/structure.png)

## Scientific Thinking
- Long interest in education, development, **assessment** (Blair, 1940; Piaget & Inhelder, 1958).
  
- Two waves of test development (Opitz, Fischer, & Heene, submitted)
    * wave I: 1970-1990.
    * wave II: since 2000s. Following PISA etc. **Rasch Application.** 
  
## Research Questions {.flexbox .vcenter}
<div class="notes">
      In general, we welcome the idea of applying the Rasch model in the field of scientific thinking.
      In the formerly used framework of classical test theory, strong measurement characteristics are assumed axiomatically and cannot be tested.
      In the Rasch model, these assumptions are parameterized, which allows strong, testable conclusions.
      However, when a new statistical tool enters a field, practices in handling the tool can affect how the tool influences the field.
      A psychometric model is a powerful but complex tool that might cause confusion about its adequate application.
      Therefore, we were interested in how researchers in scientific thinking apply the Rasch model, and how they interpret it.
</div>

1. Practices in the **application** of the Rasch Model.

2. Practices in the **interpretation** of the Rasch Model.

## The Rasch Model

<div class="notes">
      The Rasch Model is a simple model in comparison to other models from the related family of item reponse theory models.
      The the model describes that a test taker's probability to solve an item depends logarithmically only on the test taker's ability that is depicted in one person parameter [POINT AT THETA PARAMETER], and on the item's difficulty that is depicted in one item parameter [POINT AT SIGMA].
</div>

<div class="centered">

$p(x_{pi})=\frac{exp(x_{pi}(\theta_p-\sigma_i))}{{1+exp(\theta_p-\sigma_i)}}$

person ability $\theta_p$, item difficulty $\sigma_i$

![image](img/properties_1.png)

</div>
    
## The Rasch Model
<div class="notes">
      Notably, this elegant model simplicity implies strong assessment characteristics.
</div>

<div class="centered">

$p(x_{pi})=\frac{exp(x_{pi}(\theta_p-\sigma_i))}{{1+exp(\theta_p-\sigma_i)}}$

person ability $\theta_p$, item difficulty $\sigma_i$

![image](img/properties_2.png)

</div>

## The Rasch Model
<div class="notes">
If the answers on an instrument are in accordance with the Rasch model, various characteristics are given that are again interrelated and that are very favorable for research purposes and for applied measurement.
      [BULLET 1] First, a test taker's score entails all information about the assessed psychological trait, in our case about the test taker's scientific thinking ability. There is no need to take a look at which specific items the test taker solved.
       [BULLET 2] Second, it does not matter which personal characteristics a person has and which items from an instrument the person works on. Any items will lead to the same conclusion about any person's ability. Only the precision of the ability estimate depends on the choice of items and persons.
       [BULLET 3] Finally, scores from Rasch homogenous instruments can be linked between partially overlapping instruments which allows booklet designs in large scale assessments and linking test scores over time in devleopmental studies.

> - Sufficient statistics
    * All information is in the sum score.
    
> - Specific objectivity
    * Same information across any samples of persons and items.
   
> - Invariant Measurement
    * Link different tests in large-scale and longitudinal designs.
   
</div>
<div class="centered">

$p(x_{pi})=\frac{exp(x_{pi}(\theta_p-\sigma_i))}{{1+exp(\theta_p-\sigma_i)}}$

person ability $\theta_p$, item difficulty $\sigma_i$

![image](img/properties_2_details.png)

</div>  

## The Rasch Model
<div class="notes">
      However, in turn the favorable characteristics demand strong theoretical and statistical model assumptions
</div>

<div class="centered">

$p(x_{pi})=\frac{exp(x_{pi}(\theta_p-\sigma_i))}{{1+exp(\theta_p-\sigma_i)}}$

person ability $\theta_p$, item difficulty $\sigma_i$

![image](img/properties_3.png)

</div> 

## The Rasch Model: Strong Assumptions
<div class="notes">
      The assumptions of the Rasch model are all interrelated.
      [BULLET 1] Central is the assumption that only one coherent psychological trait is involved in the answer process to all items.
      [BULLET 2] Apart from this trait no other systematic influences on the item answers are present, so that the answers to the different items are stochastically independent of each other.
      [BULLET 3] For researchers who are used to applying factor analytic models to their data, particularly the assumption that all item discriminations are equal might seem especially restrictive. The Rasch model is similar to a factor model in which all factor loadings are restricted to be equal, which is crucial for the models nice characteristics.
</div>

> - Unidimensionality
    * One coherent psychological trait

> - Local (stochastic) independence
    * No additional systematic influences on answers

> - Homogenous item discriminations
    * Parallel Item Characteristic Curves (equal factor loadings)
      ![Image](img/ICCAFR_size.png)


## Practices in the Field: Rasch **Application**
<div class="notes">
      The mentioned favorable characteristics of an instrument are only given if the Rasch model holds.
      Therefore, we were interested in the practices of model fit testing in research on scientific thinking.
      Based on a recent review, we could identify 7 articles that reported instrument development using the Rasch model.
      We reviewed the model fit testing practices in these articles, with two main findings.
      First, in a few of the articels statistical tests were applied to compare measurement models with different numbers of dimensions, outfit statistics were investigated to assess model fit, or measurement invariance was investigated between groups that the researchers wanted to compare on the instrument.
      Second, more interestingly and crucially, the strategies of all of the researchers shared three characteristics. In each of the articles, a unidimensional Rasch model was estimated, always in the software ConQuest, and item infit statistics are reported and used as the main source of model fit assessment.
      The question comes up whether item infit statistics are an appropriate means of model fit testing.
| Reference               | Infit | Criterion                 | Reliability | lrt | irem | mat | Software             |
|-------------------------|-------|---------------------------|-------------|-----|------|-----|----------------------|
| Mayer at al. (2014)     | x     | -                         | EAP/PV      | -   | -    | -   | ConQuest             |
| Koerber et al. (2014)   | x     | 0.85-1.15 (-)             | EAP/PV      | -   | x    | x   | ConQuest             |
| Hartmann et  al. (2015) | x     | -                         | EAP/PV      | -   | x    | x   | ConQuest             |
| Nowak et al. (2013)     | x     | 0.8-1.2 (Adams, 2002)     | EAP/PV      | x   | x    | x   | ConQuest             |
| Grube (2010)            | x     | 0.8-1.2 (Adams, 2000)     | EAP/PV      | x   | x    | x   | ConQuest             |
| Heene (2007)            | x     | 0.8-1.2 (Wright, 2000)    | PSR/ISR     | -   | x    | -   | ConQuest, WS, FC, WM |
| Brown et al. (2010)     | x     | -                         | PSR         | -   | -    | -   | ConQuest             |
## Practices in the Field: Rasch **Application**
- 7 instruments based on Rasch model, mostly since 2010 (Review: Opitz, Fischer, & Heene, submitted)
- Typical approach is to fit **unidimensional Rasch model** in **ConQuest software**, remove items based on **infit statistics**

</div>

| Reference               | Infit | Criterion                 | Software              |
|-------------------------|-------|---------------------------|-----------------------|
| Mayer at al. (2014)     | x     | -                         |  ConQuest             |
| Koerber et al. (2014)   | x     | 0.85-1.15 (-)             |  ConQuest             |
| Hartmann et  al. (2015) | x     | -                         |  ConQuest             |
| Nowak et al. (2013)     | x     | 0.8-1.2 (Adams, 2002)     |  ConQuest             |
| Grube (2010)            | x     | 0.8-1.2 (Adams, 2000)     |  ConQuest             |
| Heene (2007)            | x     | 0.8-1.2 (Wright, 2000)    |  ConQuest, WS, FC, WM |
| Brown et al. (2010)     | x     | -                         |  ConQuest             |

## Itemfit

<div class="notes">
      Item infit statistics depict the deviation of the empirical scores from the model implied estimated scores on the level of each individual item.
      The statistics can take any positive value and should be close to 1.
      In most of the seven reviewed articles, researchers used the established cutoff values of 0.8 to 1.2 to decide whether each of their items fits the Rasch model.
      In each article, items that exceeded the cutoffs were removed from the respective instrument.
      Can this procedure be judged adequate?
      [BULLET] The literature says no. Item infit statistics are underpowered against deviations from the major assumptions of the Rasch model. In addition, they should only be applied after global model fit tests have been applied. The plain removal of non fitting items might reduce validity since usually each item is developed to cover a part of the theoretical construct of interest.
</div>


- Based on the individual response residuals
<div class="centered">
$$
\begin{align}
R_{pi} &= X_{pi} - E_{pi}\\
Z_{pi}^2 &= \frac{R_{pi}^2}{VAR(X_{pi})}\\
\end{align}
$$
</div>

> -
$$
\begin{equation}
INFIT_i = \frac{\sum^n_{p=1}R^2_{pi}}{\sum^n_{p=1}VAR(X_{pi})}
\end{equation}
$$

> - Degree of item-level deviation from expected scores given the Rasch model

## Itemfit
- Adequacy for model fit testing **strongly criticized** (Christensen & Kreiner, 2013; Heene et al., 2014; Smith, 1995; Smith, 1996; Smith et al., 1998)
    * Underpowered.
    * Should be applied only after investigation of **global model fit**.
    * Item removal might **reduce construct validity**.
    
    
> - **Item infit statistics are inadequate to evaluate the fit of the Rasch model.**
    
## Itemfit: Simulations
<div class="notes">
      We did not want to rely only on available literature.
      Therefore, we conducted simulations to examine the appropriateness of itemfit statistics in the specific scenarios from the reviewed articles.
      First, we estimated infit statistics for answer matrices that were simulated from the 2PL model, meaning that the assumption of nonoverlapping item characteristic curves or equal factor loadings is violated.
      We covered the typical item and sample sizes from the 7 Rasch articles.
      Our findings explain a lot of the current practices.
      For most item and sample sizes, infit statistics indicate misfit for between 1 and 10 items. Since we sampled all items from the 2PL model, in reality all items should be dismissed. However, based on the infit statistics, researchers will usually remove the indicated 1 to 10 items and conclude that the rest of the item pool is Rasch homogenous. This crucial mistake is related to te very nature of itemfit statistics. As their name implies, itemfit statistics indicate misfit on the item level, not on the global level of the model. Therefore, itemfit statistics can logically not lead to the rejection of the Rasch model.
      We conclude that model fit was not tested adequately in any of the reviewed Rasch articles.
</div>

- Dichotomous data
    * used R package eRm
    * Item size: 12, 24, 60, 120
    * Sample Size: 150, 300, 500, 1000
    * 1000 data sets per combination
    * Code, slides, comments at [https://github.com/dostodabsi/simrasch](https://github.com/dostodabsi/simrasch)

## Simulation I
- unequal item discrimination
- $d_i \sim \text{lognormal}(0, \sigma)$
- $\sigma$ either .1, .3, .5
    
    
## 
![res1](img/pl_plot.png)
    
## Simulation I
- Results:
    * Note that **we did not test the Rasch model**
    * We assumed that it holds and threw out certain items
    
> - Question:
    * After removal of "bad" items, does the Rasch model hold?
    * Not tested in the literature - let's simulate it!
    
## 
![res2](img/LR_plot.png)

## Simulation II
<div class="notes">
      For a second simulation scenario, we reviewed which theoretical models of scientific thinking the researchers assumed.
      Most researchers assumed between 3 and 5 correlated facets of scientific thinking.
      Therefore, we simulated data from a model with 4 correlated dimensions.
      What do itemfit statistics show when the unidimensional Rasch model is fit on these data?
      Again, they tell us to remove some of the items, for most item and sample sizes between 1 and 5 items.
      Then, from the view of infit statistics, everything is fine.
      In addition, we fould that the reliability estimation for the unidimensional model is quite low, between .5 and .6.
      Interestingly, this is what researchers find in most of the seven Rasch articles:
      Even for instruments with large numbers of items, researchers find low reliability estimates.
      We assume that this plainly stems from fitting a unidimensional model to multidimensional data.
      Reliability is in general estimated rather low in case unmodelled multidimensionality is present.
</div>      

- Typical theoretical model
    * 3-5 correlated facets of Scientific Reasoning assumed
    * Simulated factor structure:
    * $\left( \begin{array}{ccc}
        1.0 & 0.6 & 0.5 & 0.4\\
        0.6 & 1.0 & 0.7 & 0.4\\
        0.5 & 0.7 & 1.0 & 0.3\\
        0.4 & 0.4 & 0.3 & 1.0 \end{array} \right)$
    * How do infit statistics respond to such data?
    
## 
![mult](img/mult_plot.png)

## Simulation II
- Results:
    * item fit statistics **completely miss the point**
    * very few items "have to be removed"
    * Unidimensional Rasch model shows low reliability (about .5 - .6)
    
> - Conclusion:
    * **Simulated scenario yields results similar to reviewed articles**
    * **Might the simulated scenario reflect what is happening in the field?**

## Practices in the Field: **Interpretation**
<div class="notes">
      Our simulations showed that judging the unidimensional Rasch model appropriate based on item infit statistics is an inadequate practice.
      We were interested in the conclusions that researchers draw based on this practice.
      We reviewed reseachers' theoretical models, statistical models, and what the researchers concluded from fitting the Rasch model.
      We find that researchers' theoretical model, statistical model, and theoretical conclusions are often inconsistent. 
      For example, some researchers that compared various models concluded that a model with with three or four dimensions of scientific thinking fits best.
      Still, all researchers eventually estimated a unidimensional Rasch model for reliability estimation and item infit estimation.
      Some reserachers concluded based on these practices that good model data fit is present, that their instrument is suitable for measurement and fulfills the affordances of the Rasch model, and that the instrument measures a unitary construct and a unidimensional theoretical model is supported.
      From our point of view, the conclusion it due that researchers engage in inconsistent practices that lead to tautological reasoning. Basically, they assume the Rasch model, do not test it, but eventually they conclude that it fits. In this way, the a priori assumption becomes the conclusion.
    * "good model data fit, suitableness for measurement, fulfillment of psychometric affordances of Rasch model"
    * "valid treatment as a single scale"
</div>

| Reference               | Theoretical models | Fitted models | Best fit | Reliability | Itemfit |
|-------------------------|--------------------|---------------|----------|-------------|---------|
| Mayer at al. (2014)     | 4D                 | 1D            | na       | 1D          | 1D      |
| Koerber et al. (2014)   | 1D, 5D             | 1D            | na       | 1D          | 1D      |
| Hartmann et  al. (2015) | 1D                 | 1D            | na       | 1D          | 1D      |
| Nowak et al. (2013)     | 1D, 3D             | 1D, 3D        | 3D       | 1D          | 1D      |
| Grube (2010)            | 4D                 | 1D, 4D        | 4D       | 1D          | 1D      |
| Heene (2007)            | 1D                 | 1D            | na       | 1D          | 1D      |
| Brown et al. (2010)     | 1D                 | 1D            | na       | 1D          | 1D      |

## Practices in the Field: **Interpretation**
- Conclusions in reviewed articles:
    * "scale measures unitary construct, support for unidimensional theoretical model"

> - Our conclusions:
    * __Theoretical models, modeling practices, and theoretical conclusions are inconsistent.__
    * __Researchers engage in tautological and reductionist reasoning that supports their theoretical models and instrument quality.__

## Discussion
<div class="notes">
      Our findings suggest some conjecturing about the reasons why researchers engage in these practices.
      I discuss our conjectures to provide suggestions for improved future practices.
      In general, there are different approaches to Rasch modeling.
      There is the utility approach that consists mostly of estimating the model parameters and inspecting patterns in item and person fit to investigate whether an instrument is useful for applied assessments.
      On the other hand, in the hypothesis testing approach strict model fit testing procedures are applied to allow strong conclusions about an instruments accordance with the Rasch model.
      Clearly, for informing scientific theory the hypothesis testing approach seems indicated.
      However, our findings indicate that researchers follow the utility approach but eventually they draw inferences that need the hypothesis testing approach.
      We think that researchers might follow this approach because it is used in the international large scale programs such as PISA.
      They use the ConQuest software that is used for example in PISA and that does not allow extensive global model fit testing.
      Finally, this approach might be the first and only one that researchers learn.
- **The current practices are reductionist, inadequate, inconsistent, and might be detrimental for theory development in the field of Scientific Thinking**.
- *Why do researchers engage in these practices?*
</div>

- Two traditional **perspectives on Rasch**: Utility vs. hypothesis testing (cf. Linacre, 2010).
    
> - Researchers apply practices of the *utility school*. **Potential reasons?**
    * Following large scale programs
    * Limiting software (ConQuest)
    * Lack of knowledge about alternative model tests

## Suggestions
<div class="notes">
      The first suggestion that we would like to make for improved future practices is the application of global model fit testing procedures before local fit on the level of items and persons is evaluated.
      Classic tests for the Rasch model encompass the andersen test for person homogeneity and the martin l?f test for item homogeneity.
      Recently, Maydeu-Olivares provided an extensive overview of model fit testing techniques in which he points out the M2 statistics for the dichotomous Rasch model.
      For small sample sizes, nonparametric alternatives have been developed and in a Bayesian framework posterior predictive checking is possible.
      For applied researchers pleasant news might be that Maydeu-Olivares imported well known SEM fit statistics such as the RMSEA, the CFI, and the SRMR into the item response theory world.
      Finally, the software R represents a powerful and versatile tool that is freely available and has various packages that include most of these techniques.
      Our second suggestion is that researchers should think about using alternative measurement models that are perhaps less restrictive and fit their theoretical assumptions and research intentions better.
      There are various alternative parametric and nonparametric IRT models that differ in the strictness of model assumptions and provide different theoretical information.
</div>

- **Acknowledge the hypothesis testing perspective**
    * Andersen test (Andersen, 1973) 
    * Martin Löf test (Christensen et al., 2002) 
    * Recursive Partitioning (Strobl et al., 2013) 
    * M2 statistic (Maydeu-Olivares, 2013)
    * nonparametric statistics (Koller et al., 2015)
    * posterior predictive checking (Fox, 2010; Sinharay, 2006)
    * SEM-like statistics (Maydeu-Olivares, 2014) 

## Suggestions
- **Match theoretical and statistical models**
    * IRT: Multidimensional Rasch, 2PL, mixture models (Fischer & Molenaar, 1995)
    * Nonparametric IRT: mokken scaling (van der Graaf et al., 2015)
    * Applied IRT: cognitive diagnosis models (de la Torre & Minchen, 2014)

## Suggestions {.flexbox .vcenter}
![useR](img/useR.png)


## Suggestions {.flexbox .vcenter}
<div class="notes">
      Finally, we suggest to acknowledge the limits of psychometric models and to go down a grain size and look at scientific thinking again in its whole complexity.
      Looking at scientific thinking as a complex network of interrelated activities in the real world but also psychologically might be more fruitful for theory development than reductionist models.
</div>

- **Acknowledge complexity**
    * Network models (Schmittman et al., 2013)
    * Longitudinal and experimental designs to examine interactions
![structure](img/structure.png)

## Thanks! {.flexbox .vcenter}
[https://github.com/dostodabsi/simrasch](https://github.com/dostodabsi/simrasch)
